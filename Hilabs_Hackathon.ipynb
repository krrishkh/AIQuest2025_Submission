{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ify3tXXHTR_z",
        "outputId": "2ce68688-29e9-4ff6-cb29-fafffca944ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.0/1.5 MB 279.3 kB/s eta 0:00:06\n",
            "   - -------------------------------------- 0.0/1.5 MB 279.3 kB/s eta 0:00:06\n",
            "   -- ------------------------------------- 0.1/1.5 MB 403.5 kB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.1/1.5 MB 435.7 kB/s eta 0:00:04\n",
            "   ------ --------------------------------- 0.2/1.5 MB 752.5 kB/s eta 0:00:02\n",
            "   ------ --------------------------------- 0.2/1.5 MB 752.5 kB/s eta 0:00:02\n",
            "   ------------ --------------------------- 0.5/1.5 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.5/1.5 MB 1.2 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 0.8/1.5 MB 1.6 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 0.9/1.5 MB 1.7 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 0.9/1.5 MB 1.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.4/1.5 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.4/1.5 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.5/1.5 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 2.0 MB/s eta 0:00:00\n",
            "Installing collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.14.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7OJvd4H3T2Ru"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Provider Specialty → NUCC Taxonomy Ensemble Mapper\n",
        "- No external API calls. Optional local embeddings if installed (sentence-transformers).\n",
        "- Combines: Synonyms → Fuzzy → TF-IDF → (optional) Embeddings, with calibrated ensemble scoring.\n",
        "- Handles multi-specialty strings (e.g., \"Cardio / Diab\", \"ENT & Allergy\").\n",
        "- Returns JUNK if confidence below threshold.\n",
        "\n",
        "Usage:\n",
        "  This script is designed to be run directly in a Python environment or imported as a module.\n",
        "  If running as a script with command-line arguments, use the --nucc, --input, --out, etc. flags.\n",
        "  If importing as a module, call the main function with appropriate arguments.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz, process\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# --------------------------- Text Utilities ---------------------------\n",
        "\n",
        "PUNCT_RE = re.compile(r\"[^a-z0-9&/ +\\-]\")\n",
        "SPACE_RE = re.compile(r\"\\s+\")\n",
        "SPLIT_MULTI = re.compile(r\"[;/,&+]| and | with \", flags=re.IGNORECASE)\n",
        "JUNK_HINTS = re.compile(r\"\\b(dept|department|clinic|hospital|desk|admin|billing|front|room|floor|block)\\b\")\n",
        "\n",
        "def normalize_text(t: str) -> str:\n",
        "    t = str(t).lower().strip()\n",
        "    t = PUNCT_RE.sub(\" \", t)\n",
        "    t = SPACE_RE.sub(\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def expand_synonyms(text: str, syn_map: Dict[str, str]) -> str:\n",
        "    if not syn_map:\n",
        "        return text\n",
        "    parts = text.split()\n",
        "    return \" \".join([syn_map.get(w, w) for w in parts])\n",
        "\n",
        "def split_multi_specialty(raw: str) -> List[str]:\n",
        "    # Split on separators but keep compact chunks\n",
        "    chunks = [c.strip() for c in SPLIT_MULTI.split(str(raw)) if c.strip()] # Ensure raw is a string\n",
        "    # Fall back to original if split produced nothing useful\n",
        "    return chunks or [str(raw)]\n",
        "\n",
        "\n",
        "# --------------------------- Embeddings (Optional) ---------------------------\n",
        "\n",
        "def try_load_embedder(use_embeddings: bool, model_name: str):\n",
        "    if not use_embeddings:\n",
        "        return None, None\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer(model_name)\n",
        "        return model, model.get_sentence_embedding_dimension()\n",
        "    except Exception as e:\n",
        "        print(f\"[INFO] Embeddings disabled (could not init '{model_name}'): {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --------------------------- Ensemble Scoring ---------------------------\n",
        "\n",
        "def rescale_0_1(x, lo, hi):\n",
        "    # Clamp/linear rescale\n",
        "    x = max(lo, min(hi, x))\n",
        "    return (x - lo) / (hi - lo) if hi > lo else 0.0\n",
        "\n",
        "def combine_scores(fuzzy, tfidf, embed, w_fuzzy=0.45, w_tfidf=0.35, w_embed=0.20) -> float:\n",
        "    # If embeddings are missing, renormalize weights\n",
        "    if embed is None:\n",
        "        s = w_fuzzy + w_tfidf\n",
        "        w_fuzzy /= s\n",
        "        w_tfidf /= s\n",
        "        return w_fuzzy * fuzzy + w_tfidf * tfidf\n",
        "    return w_fuzzy * fuzzy + w_tfidf * tfidf + w_embed * embed\n",
        "\n",
        "\n",
        "# --------------------------- Main Pipeline ---------------------------\n",
        "\n",
        "def load_synonyms(path: str) -> Dict[str, str]:\n",
        "    if not path or not os.path.exists(path):\n",
        "        # Default medical shorthand expansions\n",
        "        return {\n",
        "            \"ent\": \"otolaryngology\",\n",
        "            \"obgyn\": \"obstetrics gynecology\",\n",
        "            \"gyn\": \"gynecology\",\n",
        "            \"cardio\": \"cardiology\",\n",
        "            \"cv\": \"cardiovascular\",\n",
        "            \"endo\": \"endocrinology\",\n",
        "            \"diab\": \"diabetes\",\n",
        "            \"neuro\": \"neurology\",\n",
        "            \"derma\": \"dermatology\",\n",
        "            \"ortho\": \"orthopedic\",\n",
        "            \"psych\": \"psychiatry\",\n",
        "            \"ped\": \"pediatrics\",\n",
        "            \"peds\": \"pediatrics\",\n",
        "            \"heent\": \"otolaryngology\",\n",
        "            \"audiologist\": \"audiology\",\n",
        "            \"path\": \"pathology\",\n",
        "            \"im\": \"internal medicine\",\n",
        "            \"fp\": \"family medicine\",\n",
        "            \"pmr\": \"physical medicine rehabilitation\",\n",
        "        }\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna()\n",
        "    # Expected columns: short_form, standard_term\n",
        "    m = {}\n",
        "    for _, r in df.iterrows():\n",
        "        m[str(r[0]).strip().lower()] = str(r[1]).strip().lower()\n",
        "    return m\n",
        "\n",
        "\n",
        "def build_nucc_corpus(nucc: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    # Build a combined searchable string per taxonomy row\n",
        "    for col in [\"Code\", \"Grouping\", \"Classification\", \"Specialization\", \"Display_Name\", \"Definition\", \"Notes\", \"Section\"]:\n",
        "        if col not in nucc.columns:\n",
        "            nucc[col] = \"\"\n",
        "    nucc = nucc.fillna(\"\")\n",
        "    nucc[\"combined\"] = (nucc[\"Classification\"] + \" \" +\n",
        "                        nucc[\"Specialization\"] + \" \" +\n",
        "                        nucc[\"Display_Name\"]).map(normalize_text)\n",
        "    # Fallback: if combined is empty, use grouping/definition\n",
        "    mask_empty = nucc[\"combined\"].str.len() == 0\n",
        "    nucc.loc[mask_empty, \"combined\"] = (nucc.loc[mask_empty, \"Grouping\"] + \" \" +\n",
        "                                        nucc.loc[mask_empty, \"Definition\"]).map(normalize_text)\n",
        "    return nucc, nucc[\"combined\"].tolist()\n",
        "\n",
        "\n",
        "def precompute_models(corpus: List[str], use_embeddings: bool, model_name: str):\n",
        "    # TF-IDF\n",
        "    tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "    tfidf_matrix = tfidf.fit_transform(corpus)\n",
        "\n",
        "    # Embeddings optional\n",
        "    embedder, _ = try_load_embedder(use_embeddings, model_name)\n",
        "    if embedder is not None:\n",
        "        emb_matrix = embedder.encode(corpus, show_progress_bar=False, normalize_embeddings=True)\n",
        "    else:\n",
        "        emb_matrix = None\n",
        "\n",
        "    return tfidf, tfidf_matrix, embedder, emb_matrix\n",
        "\n",
        "\n",
        "def shortlist_candidates(query: str, corpus: List[str], topk: int) -> List[int]:\n",
        "    # Rapid shortlist using fuzzy WRatio top-k\n",
        "    # Returns indices of candidates inside corpus\n",
        "    # Using process.extract provides (match_str, score, index)\n",
        "    results = process.extract(query, corpus, scorer=fuzz.WRatio, limit=topk)\n",
        "    idxs = [r[2] for r in results]\n",
        "    return idxs\n",
        "\n",
        "\n",
        "def score_against_candidates(query: str,\n",
        "                             nucc: pd.DataFrame,\n",
        "                             corpus: List[str],\n",
        "                             cand_idx: List[int],\n",
        "                             tfidf, tfidf_matrix,\n",
        "                             embedder, emb_matrix):\n",
        "    # Fuzzy scores (scaled to 0..1)\n",
        "    fuzzy_scores = []\n",
        "    for i in cand_idx:\n",
        "        s = corpus[i]\n",
        "        # Blend several fuzzy scorers to reduce idiosyncrasies\n",
        "        w = fuzz.WRatio(query, s)\n",
        "        ts = fuzz.token_sort_ratio(query, s)\n",
        "        pr = fuzz.partial_ratio(query, s)\n",
        "        f_blend = 0.5 * w + 0.3 * ts + 0.2 * pr     # in 0..100\n",
        "        fuzzy_scores.append(rescale_0_1(f_blend, 40, 100))  # soften scaling\n",
        "\n",
        "    # TF-IDF score\n",
        "    q_vec = tfidf.transform([query])\n",
        "    cos = cosine_similarity(q_vec, tfidf_matrix[cand_idx]).ravel()\n",
        "    tfidf_scores = [rescale_0_1(float(x), 0.15, 0.75) for x in cos]  # empirical scaling\n",
        "\n",
        "    # Embedding score (optional)\n",
        "    if embedder is not None and emb_matrix is not None:\n",
        "        q_emb = embedder.encode([query], show_progress_bar=False, normalize_embeddings=True)[0]\n",
        "        emb_scores = np.dot(emb_matrix[cand_idx], q_emb).tolist()   # cosine in [-1,1]\n",
        "        emb_scores = [rescale_0_1((x + 1) / 2.0, 0.55, 0.95) for x in emb_scores]  # map to 0..1 with floor\n",
        "    else:\n",
        "        emb_scores = [None] * len(cand_idx)\n",
        "\n",
        "    # Combine\n",
        "    combined = []\n",
        "    for k, i in enumerate(cand_idx):\n",
        "        conf = combine_scores(fuzzy_scores[k], tfidf_scores[k], emb_scores[k])\n",
        "        combined.append((i, conf, fuzzy_scores[k], tfidf_scores[k], (emb_scores[k] if emb_scores[k] is not None else -1.0)))\n",
        "    # Sort by combined confidence desc\n",
        "    combined.sort(key=lambda t: t[1], reverse=True)\n",
        "    return combined\n",
        "\n",
        "\n",
        "def map_one(raw: str,\n",
        "            nucc: pd.DataFrame,\n",
        "            corpus: List[str],\n",
        "            tfidf, tfidf_matrix,\n",
        "            embedder, emb_matrix,\n",
        "            syn_map: Dict[str, str],\n",
        "            topk: int,\n",
        "            threshold: float) -> Tuple[str, float, str]:\n",
        "    if not raw or str(raw).strip() == \"\":\n",
        "        return \"JUNK\", 0.0, \"Empty input\"\n",
        "\n",
        "    original = str(raw)\n",
        "    # Heuristic early junk detection\n",
        "    if JUNK_HINTS.search(original.lower()):\n",
        "        # keep processing, but mark if it falls low\n",
        "        pass\n",
        "\n",
        "    # Normalize & expand\n",
        "    cleaned = normalize_text(original)\n",
        "    cleaned = expand_synonyms(cleaned, syn_map)\n",
        "\n",
        "    # Short-circuit: if query exactly equals any \"Classification\" token, boost later via fuzzy\n",
        "    # Shortlist\n",
        "    cand_idx = shortlist_candidates(cleaned, corpus, topk=topk)\n",
        "    if not cand_idx:\n",
        "        return \"JUNK\", 0.0, \"No candidates\"\n",
        "\n",
        "    # Score candidates with ensemble\n",
        "    combined = score_against_candidates(cleaned, nucc, corpus, cand_idx,\n",
        "                                        tfidf, tfidf_matrix, embedder, emb_matrix)\n",
        "\n",
        "    # Take top score and include ties within a small delta\n",
        "    if not combined:\n",
        "        return \"JUNK\", 0.0, \"No scores\"\n",
        "\n",
        "    best_conf = combined[0][1]\n",
        "    if best_conf < threshold:\n",
        "        return \"JUNK\", round(float(best_conf), 2), f\"No confident match (score={best_conf:.2f})\"\n",
        "\n",
        "    # Gather ties within 0.03 for ambiguity\n",
        "    tie_delta = 0.03\n",
        "    picks = [combined[0]]\n",
        "    for item in combined[1:]:\n",
        "        if best_conf - item[1] <= tie_delta and item[1] >= threshold:\n",
        "            picks.append(item)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Convert to codes and explanation\n",
        "    codes = []\n",
        "    expl_parts = []\n",
        "    for idx, conf, fz, tf, em in picks:\n",
        "        row = nucc.iloc[idx]\n",
        "        codes.append(str(row[\"Code\"]))\n",
        "        name = row[\"Display_Name\"] if \"Display_Name\" in row and str(row[\"Display_Name\"]).strip() else row[\"Classification\"]\n",
        "        em_txt = f\"{em:.2f}\" if em >= 0 else \"NA\"\n",
        "        expl_parts.append(f\"{name} [ens={conf:.2f}, fuzzy={fz:.2f}, tfidf={tf:.2f}, emb={em_txt}]\")\n",
        "\n",
        "    nucc_codes = \" | \".join(codes)\n",
        "    explain = f\"Matched '{original}' → \" + \" ; \".join(expl_parts)\n",
        "    return nucc_codes, round(float(best_conf), 2), explain\n",
        "\n",
        "\n",
        "def map_row_multispecialty(raw, **kwargs): # Removed type hint for raw\n",
        "    # Split multi-specialty strings and union the results\n",
        "    parts = split_multi_specialty(raw)\n",
        "    all_codes = []\n",
        "    explains = []\n",
        "    confs = []\n",
        "\n",
        "    for p in parts:\n",
        "        codes, conf, exp = map_one(p, **kwargs)\n",
        "        confs.append(conf)\n",
        "        explains.append(exp)\n",
        "        if codes != \"JUNK\":\n",
        "            all_codes.extend([c.strip() for c in codes.split(\"|\")])\n",
        "\n",
        "    if not all_codes:\n",
        "        # if none mapped, return the best single part explanation\n",
        "        best_i = int(np.argmax(confs))\n",
        "        return \"JUNK\", float(np.max(confs)), f\"Multi-part unresolved; best part: {explains[best_i]}\"\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    uniq_codes = []\n",
        "    for c in all_codes:\n",
        "        if c not in seen and c != \"\":\n",
        "            seen.add(c)\n",
        "            uniq_codes.append(c)\n",
        "\n",
        "    # Confidence for multi-specialty = max part confidence\n",
        "    final_conf = float(np.max(confs))\n",
        "    return \" | \".join(uniq_codes), round(final_conf, 2), \" + \".join(explains)\n",
        "\n",
        "\n",
        "def main(nucc_path: str, input_path: str, out_path: str, syn_path: str = \"\", threshold: float = 0.50, topk: int = 25, use_embeddings: bool = False, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "    # Load data\n",
        "    nucc = pd.read_csv(nucc_path)\n",
        "    if \"raw_specialty\" not in pd.read_csv(input_path, nrows=1).columns.tolist():\n",
        "        print(\"[ERROR] 'raw_specialty' column not found in input CSV.\", file=sys.stderr)\n",
        "        sys.exit(2)\n",
        "    input_df = pd.read_csv(input_path)\n",
        "\n",
        "    # Build NUCC corpus\n",
        "    nucc, corpus = build_nucc_corpus(nucc)\n",
        "\n",
        "    # Prepare models\n",
        "    tfidf, tfidf_matrix, embedder, emb_matrix = precompute_models(\n",
        "        corpus, use_embeddings=use_embeddings, model_name=model_name\n",
        "    )\n",
        "\n",
        "    # Synonyms\n",
        "    syn_map = load_synonyms(syn_path)\n",
        "\n",
        "    # Map rows\n",
        "    out_rows = []\n",
        "    for raw in input_df[\"raw_specialty\"].tolist():\n",
        "        codes, conf, explain = map_row_multispecialty(\n",
        "            raw=raw,\n",
        "            nucc=nucc,\n",
        "            corpus=corpus,\n",
        "            tfidf=tfidf,\n",
        "            tfidf_matrix=tfidf_matrix,\n",
        "            embedder=embedder,\n",
        "            emb_matrix=emb_matrix,\n",
        "            syn_map=syn_map,\n",
        "            topk=topk,\n",
        "            threshold=threshold,\n",
        "        )\n",
        "        out_rows.append((raw, codes if codes else \"JUNK\", conf, explain))\n",
        "\n",
        "    out_df = pd.DataFrame(out_rows, columns=[\"raw_specialty\", \"nucc_codes\", \"confidence\", \"explain\"])\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    print(f\"[OK] Wrote {len(out_df)} rows to {out_path}\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # This part is commented out to avoid argparse errors when running in the notebook\n",
        "#     # ap = argparse.ArgumentParser()\n",
        "#     # ap.add_argument(\"--nucc\", required=True, help=\"Path to nucc_taxonomy_master.csv\")\n",
        "#     # ap.add_argument(\"--input\", required=True, help=\"Path to input_specialties.csv with column 'raw_specialty'\")\n",
        "#     # ap.add_argument(\"--out\", required=True, help=\"Path to write output CSV\")\n",
        "#     # ap.add_argument(\"--syn\", default=\"\", help=\"Optional synonyms CSV (short_form,standard_term)\")\n",
        "#     # ap.add_argument(\"--threshold\", type=float, default=0.50, help=\"Confidence threshold for non-JUNK\")\n",
        "#     # ap.add_argument(\"--topk\", type=int, default=25, help=\"Candidate shortlist size\")\n",
        "#     # ap.add_argument(\"--use-embeddings\", action=\"store_true\", help=\"Enable sentence embeddings if available\")\n",
        "#     # ap.add_argument(\"--model\", default=\"all-MiniLM-L6-v2\", help=\"SentenceTransformer model name\")\n",
        "#     # args = ap.parse_args()\n",
        "#     # main(args.nucc, args.input, args.out, args.syn, args.threshold, args.topk, args.use_embeddings, args.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvDJs-RaTo3P",
        "outputId": "4fbdf743-21e4-4822-a391-53b1e917b6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running classification for preprocessed_input.csv ...\n",
            "\n",
            "[OK] Wrote 9689 rows to classified_output_full.csv\n",
            "\n",
            "✅ Classified CSV successfully generated → classified_output_full.csv\n",
            "                     raw_specialty                            nucc_codes  \\\n",
            "0                      acupuncture                                  JUNK   \n",
            "1              adolescent medicine  2080A0000X | 207QA0000X | 207RA0000X   \n",
            "2               allergy/immunology               207KA0200X | 207K00000X   \n",
            "3      anatomic clinical pathology               207ZP0101X | 207ZP0102X   \n",
            "4                   anesthesiology                            207L00000X   \n",
            "5  applied behavioral analysis aba                            2080P0006X   \n",
            "6                        audiology                            2355A2700X   \n",
            "7                bariatric surgery                            2086S0120X   \n",
            "8        cardiac electrophysiology                            207RC0001X   \n",
            "9                  cardiac surgery                            208600000X   \n",
            "\n",
            "   confidence                                            explain  \n",
            "0        0.35  Multi-part unresolved; best part: No confident...  \n",
            "1        0.79  Matched 'adolescent medicine' → Pediatric Adol...  \n",
            "2        0.75  Matched 'allergy' → Allergy Physician [ens=0.7...  \n",
            "3        0.75  Matched 'anatomic clinical pathology' → Anatom...  \n",
            "4        0.82  Matched 'anesthesiology' → Anesthesiology Phys...  \n",
            "5        0.53  Matched 'applied behavioral analysis aba' → De...  \n",
            "6        0.74  Matched 'audiology' → Audiology Assistant [ens...  \n",
            "7        0.75  Matched 'bariatric surgery' → Pediatric Surger...  \n",
            "8        0.79  Matched 'cardiac electrophysiology' → Clinical...  \n",
            "9        0.58  Matched 'cardiac surgery' → Surgery Physician ...  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "# Import from your preprocessing script\n",
        "from preprocessing import load_synonyms, PreprocessSpecialty\n",
        "\n",
        "# --- File paths ---\n",
        "nucc_path = \"nucc_taxonomy_master.csv\"\n",
        "input_path = \"input_specialties.csv\"\n",
        "output_path = \"output_notebook.csv\"\n",
        "synonyms_path = \"synonyms.csv\"  \n",
        "\n",
        "# --- Step 1: Load input ---\n",
        "df_input = pd.read_csv(input_path)\n",
        "\n",
        "# --- Step 2: Initialize preprocessor ---\n",
        "synonyms = load_synonyms(synonyms_path)\n",
        "pre = PreprocessSpecialty(synonyms_map=synonyms)\n",
        "\n",
        "# --- Step 3: Apply preprocessing ---\n",
        "processed_records = []\n",
        "for raw in df_input.iloc[:, 0].astype(str):\n",
        "    processed, is_junk = pre.process_one(raw)\n",
        "    # If junk → skip\n",
        "    processed_records.append(None if is_junk else processed)\n",
        "\n",
        "# Replace original specialties with processed ones\n",
        "df_input.iloc[:, 0] = processed_records\n",
        "df_input = df_input.dropna().reset_index(drop=True)\n",
        "\n",
        "# --- Step 4: Save preprocessed file ---\n",
        "preprocessed_path = \"preprocessed_input.csv\"\n",
        "df_input.to_csv(preprocessed_path, index=False)\n",
        "\n",
        "# --- Step 5: Run classification on preprocessed specialties ---\n",
        "print(f\"Running classification for {preprocessed_path} ...\\n\")\n",
        "\n",
        "main(\n",
        "    nucc_path=nucc_path,\n",
        "    input_path=preprocessed_path,\n",
        "    out_path=output_path,\n",
        "    threshold=0.5,\n",
        "    topk=25,\n",
        ")\n",
        "\n",
        "# --- Step 6: Verify output ---\n",
        "if os.path.exists(output_path):\n",
        "    print(f\"\\n✅ Classified CSV successfully generated → {output_path}\")\n",
        "    df = pd.read_csv(output_path)\n",
        "    print(df.head(10))\n",
        "else:\n",
        "    print(\"❌ Output file not found. Please check logs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
